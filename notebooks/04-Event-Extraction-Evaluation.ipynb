{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Evaluation: Event Extraction\n",
    "This notebook evaluate the test set for the task `event extraction`.\n",
    "\n",
    "**Note**: Before conducting experiments, you need to install `kaner` package first. Otherwise, this notebook will raise an *import error*.\n",
    "\n",
    "```bash\n",
    "cd ../\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from kaner.context import GlobalContext as gctx\n",
    "from kaner.adapter.tokenizer import CharTokenizer\n",
    "from kaner.adapter.knowledge import Gazetteer\n",
    "from kaner.trainer import NERTrainer, TrainerConfig\n",
    "from kaner.common import load_json, load_jsonl\n",
    "\n",
    "\n",
    "gctx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer(model_folder: str) -> NERTrainer:\n",
    "    options = load_json(\"utf-8\", model_folder, \"config.json\")\n",
    "    options[\"output_folder\"] = model_folder\n",
    "    options[\"identity\"] = os.path.basename(os.path.normpath(model_folder))\n",
    "    config = TrainerConfig(options, data_folder=\"../data\")\n",
    "    tokenizer = CharTokenizer(model_folder)\n",
    "    gazetteer = Gazetteer(model_folder)\n",
    "    out_adapter = gctx.create_outadapter(config.out_adapter, dataset_folder=model_folder, file_name=\"labels\")\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=[], tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for _ in range(3)\n",
    "    )\n",
    "    collate_fn = gctx.create_batcher(\n",
    "        config.model, input_pad=tokenizer.pad_id, output_pad=out_adapter.unk_id, lexicon_pad=gazetteer.pad_id, device=config.device\n",
    "    )\n",
    "    model = gctx.create_model(config.model, **config.hyperparameters)\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def predict(trainer: NERTrainer, text: str) -> dict:\n",
    "    max_seq_len = 510\n",
    "    max_query_len = 128\n",
    "    back_offset = 32\n",
    "    max_seq_len -= max_query_len\n",
    "    # cut document\n",
    "    fragments = []\n",
    "    pointer = 0\n",
    "    while pointer < len(text):\n",
    "        new_text = text[pointer: pointer + max_seq_len]\n",
    "        fragments.append(new_text)\n",
    "        pointer += max_seq_len - back_offset\n",
    "    # predict\n",
    "    raw_result = trainer.predict(fragments)\n",
    "    result = {\n",
    "        \"text\": text,\n",
    "        \"spans\": []\n",
    "    }\n",
    "    offset = 0\n",
    "    for i in range(len(fragments)):\n",
    "        for j in range(len(raw_result[i][\"spans\"])):\n",
    "            raw_result[i][\"spans\"][j][\"start\"] += offset\n",
    "            raw_result[i][\"spans\"][j][\"end\"] += offset\n",
    "        result[\"spans\"].extend(raw_result[i][\"spans\"])\n",
    "        offset += (max_seq_len - back_offset)\n",
    "    # check\n",
    "    for span in result[\"spans\"]:\n",
    "        assert result[\"text\"][span[\"start\"]: span[\"end\"] + 1] == span[\"text\"]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon embedding is None! ../data/logs/trainer-plmtg-ccksee-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Evaluation (testset): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 395/395 [00:26<00:00, 14.65it/s]\n"
     ]
    }
   ],
   "source": [
    "datarf = load_jsonl(\"utf-8\", \"../data\", \"datahub\", \"ccksee\", \"datarf.jsonl\")\n",
    "datarf_ne = []\n",
    "for datapoint in datarf:\n",
    "    if len(datapoint[\"events\"]) > -1:\n",
    "        datarf_ne.append(datapoint)\n",
    "test_len = int(len(datarf_ne) * 0.1)\n",
    "test_data = datarf_ne[len(datarf_ne) - test_len:]\n",
    "\n",
    "trainder = load_trainer(\"../data/logs/trainer-plmtg-ccksee-1\")\n",
    "for i in tqdm.tqdm(range(len(test_data)), \"Evaluation (testset)\"):\n",
    "    test_data[i][\"predicted_spans\"] = predict(trainder, test_data[i][\"text\"])[\"spans\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Event Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.6906187624750499, 'p': 0.7966231772831927, 'r': 0.6095126247798004}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def merge_entity(data: List[dict]) -> List[dict]:\n",
    "    data = deepcopy(data)\n",
    "    for i, _ in enumerate(data):\n",
    "        # merge entity\n",
    "        events = {}\n",
    "        for span in data[i][\"predicted_spans\"]:\n",
    "            event_type, role_name = span[\"label\"].split(\".\")\n",
    "            if event_type not in events.keys():\n",
    "                events[event_type] = {}\n",
    "            if role_name not in events[event_type].keys():\n",
    "                events[event_type][role_name] = []\n",
    "            span.pop(\"label\")\n",
    "            events[event_type][role_name].append(span)\n",
    "        data[i][\"predictions\"] = events\n",
    "        \n",
    "        data[i][\"merged_predictions\"] = []\n",
    "        # default: one event only for each event type\n",
    "        # 1) compare count\n",
    "        # 2) compare probability\n",
    "        for event_type, event in events.items():\n",
    "            final_event = {\"event_type\": event_type, \"arguments\": {}}\n",
    "            for role_name, entities in events[event_type].items():\n",
    "                role_confidence = defaultdict(float)\n",
    "                role_count = defaultdict(int)\n",
    "                for entity in entities:\n",
    "                    role_confidence[entity[\"text\"]] += entity[\"confidence\"]\n",
    "                    role_count[entity[\"text\"]] += 1\n",
    "                for role_value in role_confidence.keys():\n",
    "                    role_confidence[role_value] /= role_count[role_value]\n",
    "                role_with_max_count, max_confidence = \"\", 0.\n",
    "                for role_value in role_count.keys():\n",
    "                    if role_with_max_count == \"\" or role_count[role_with_max_count] < role_count[role_value]:\n",
    "                        role_with_max_count = role_value\n",
    "                    elif role_count[role_with_max_count] == role_count[role_value] and role_confidence[role_with_max_count] < role_confidence[role_value]:\n",
    "                        role_with_max_count = role_value\n",
    "                final_event[\"arguments\"][role_name] = role_with_max_count\n",
    "            data[i][\"merged_predictions\"].append(final_event)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def evaluate(data: List[dict]) -> dict:\n",
    "    n_correct, n_predicted, n_goldtruth = 0, 0, 0\n",
    "    for i, _ in enumerate(data):\n",
    "        predicted = deepcopy(data[i][\"merged_predictions\"])\n",
    "        goldtruth = deepcopy(data[i][\"events\"])\n",
    "        for event in predicted:\n",
    "            n_predicted += len(event[\"arguments\"])\n",
    "        for event in goldtruth:\n",
    "            n_goldtruth += len(event[\"arguments\"])\n",
    "        # extract entities\n",
    "        for gold_event in goldtruth:\n",
    "            matched_count = 0\n",
    "            matched_index = -1\n",
    "            gold_roles = set(\n",
    "                [\n",
    "                    \"{0}#{1}\".format(gold_event[\"arguments\"][i][\"label\"], gold_event[\"arguments\"][i][\"text\"])\n",
    "                    for i, _ in enumerate(gold_event[\"arguments\"])\n",
    "                ]\n",
    "            )\n",
    "            for i, pred_event in enumerate(predicted):\n",
    "                pred_roles = set([\"{0}#{1}\".format(role, pred_event[\"arguments\"][role]) for role in pred_event[\"arguments\"].keys()])\n",
    "                correct_roles = gold_roles.intersection(pred_roles)\n",
    "                if matched_index == -1 or len(correct_roles) > matched_count:\n",
    "                    matched_count = len(correct_roles)\n",
    "                    matched_index = i\n",
    "            if matched_index != -1:\n",
    "                n_correct += matched_count\n",
    "                predicted.pop(matched_index)\n",
    "    p = n_correct / n_predicted if n_predicted > 0 else 0\n",
    "    r = n_correct / n_goldtruth if n_goldtruth > 0 else 0\n",
    "    f1 = 2*p*r / (p+r) if p+r > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1, \"p\": p, \"r\": r\n",
    "    }\n",
    "\n",
    "\n",
    "merged_data = merge_entity(test_data)\n",
    "print(evaluate(merged_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
