{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Experiment: Gazetteer Modification\n",
    "This notebook conducts various experiments on the gazetteer modification. We modify all lexicons of the gazetteer as all entities in the `train set` and `dev set`.\n",
    "\n",
    "**Note**: Before conducting experiments, you need to install `kaner` package first. Otherwise, this notebook will raise an *import error*.\n",
    "\n",
    "```bash\n",
    "cd ../\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from kaner.context import GlobalContext as gctx\n",
    "from kaner.adapter.in_adapter import split_dataset\n",
    "from kaner.adapter.out_adapter import BaseOutAdapter\n",
    "from kaner.adapter.tokenizer import CharTokenizer\n",
    "from kaner.adapter.knowledge import Gazetteer\n",
    "from kaner.trainer import NERTrainer, TrainerConfig\n",
    "from kaner.tracker import NERTrackerRow, NERTracker\n",
    "from kaner.common.func import query_time\n",
    "\n",
    "\n",
    "gctx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Define `trainall` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gazetteer_modification(config: TrainerConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a configuration, train a model on a dataset with gazetteer modification.\n",
    "\n",
    "    Args:\n",
    "        config (TrainerConfig): Trainer Configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_hyperparameters(tokenizer: CharTokenizer, out_adapter: BaseOutAdapter, gazetteer: Gazetteer):\n",
    "        \"\"\"\n",
    "        Update hyper parameters.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (CharTokenizer): Tokenizer.\n",
    "            out_adapter (BaseOutAdapter): Output adapter.\n",
    "            gazetteer (Gazetteer): Gazetteer.\n",
    "        \"\"\"\n",
    "        partial_configs = {\"n_tags\": len(out_adapter)}\n",
    "        partial_configs.update(tokenizer.configs())\n",
    "        partial_configs.update(gazetteer.configs())\n",
    "\n",
    "        return partial_configs\n",
    "\n",
    "    raw_datasets = split_dataset(config.dataset_folder, dataset_pp=config.dataset_pp)\n",
    "    tokenizer = CharTokenizer(config.tokenizer_model_folder)\n",
    "    tokenizer.save(config.output_folder)\n",
    "    gazetteer = Gazetteer(config.gazetteer_model_folder)\n",
    "    lexicons = [\"[PAD]\\tEntity\\tDataset{0}:train+dev\".format(config.dataset)]\n",
    "    lexicons_without_repeating = set()\n",
    "    for sample in raw_datasets[0] + raw_datasets[1]:\n",
    "        for span in sample[\"spans\"]:\n",
    "            lexicons_without_repeating.add(\"{0}\\t{1}\\tDataset{1}:train+dev\".format(span[\"text\"], span[\"label\"], config.dataset))\n",
    "            # lexicons_without_repeating.add(\"{0}\\tEntity\\tDataset{1}:train+dev\".format(span[\"text\"], config.dataset))\n",
    "    gazetteer.update(lexicons + list(lexicons_without_repeating))\n",
    "    gazetteer.save(config.output_folder)\n",
    "    out_adapter = gctx.create_outadapter(config.out_adapter, dataset_folder=config.dataset_folder, file_name=\"labels\")\n",
    "    out_adapter.save(config.output_folder, \"labels\")\n",
    "    for raw_dataset in raw_datasets:\n",
    "        gazetteer.count_freq(raw_dataset)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in raw_datasets\n",
    "    )\n",
    "    token_embeddings = tokenizer.embeddings()\n",
    "    lexicon_embeddings = gazetteer.embeddings()\n",
    "    config.hyperparameters = update_hyperparameters(tokenizer, out_adapter, gazetteer)\n",
    "    collate_fn = gctx.create_batcher(\n",
    "        config.model, input_pad=tokenizer.pad_id, output_pad=out_adapter.unk_id, lexicon_pad=gazetteer.pad_id, device=config.device\n",
    "    )\n",
    "    model = gctx.create_model(config.model, **config.hyperparameters, token_embeddings=token_embeddings, lexicon_embeddings=lexicon_embeddings)\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, nn.CrossEntropyLoss(),\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    results = trainer.train()\n",
    "\n",
    "    return results, trainer\n",
    "\n",
    "\n",
    "def trainall(labpath: str, cfgdir: str, m: List[str], d: List[str], n: int, tag: str, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Experiments for all model's training.\n",
    "\n",
    "    Args:\n",
    "        labpath (str): The file path of recording experimental results.\n",
    "        cfgdir (str): Configuration folder.\n",
    "        m (List[str]): All specific models to be trained.\n",
    "        d (List[str]): All specific datasets to be tested.\n",
    "        n (int): The number of training repeating times.\n",
    "        tag (str): Experimental tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_names(names: List[str], all_names: List[str], name_type: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Check whether the name that user inputs is correct.\n",
    "\n",
    "        Args:\n",
    "            names (List[str]): The names (dataset, model, gazetteer) that user inputs.\n",
    "            all_names (List[str]): All names (dataset, model, gazetteer) that this libary provides.\n",
    "            name_type (str): The type of the name (Dataset, Model, Gazetteer).\n",
    "        \"\"\"\n",
    "        if len(names) == 0:\n",
    "            names = all_names\n",
    "        else:\n",
    "            for name in names:\n",
    "                if name not in all_names:\n",
    "                    print(\"[{0}] {1} is not in {2}\".format(name_type, name, all_names))\n",
    "                    exit(0)\n",
    "        return names\n",
    "\n",
    "    tracker = NERTracker.load(labpath)\n",
    "    models = update_names(m, gctx.get_model_names(), \"Model\")\n",
    "    datasets = update_names(d, gctx.get_dataset_names(), \"Dataset\")\n",
    "\n",
    "    print(\"--------------------- Laboratory Configuration ---------------------\")\n",
    "    print(\"Models: {0}\".format(models))\n",
    "    print(\"Datasets: {0}\".format(datasets))\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            for _ in range(n):\n",
    "                if len(tracker.query(dataset=dataset, model=model, tag=tag)) >= n:\n",
    "                    continue\n",
    "                config = TrainerConfig(os.path.join(cfgdir, model + \".yml\"), dataset=dataset, **kwargs)\n",
    "                start = str(datetime.now())\n",
    "                try:\n",
    "                    results, trainer = train_with_gazetteer_modification(config)\n",
    "                except RuntimeError as error:\n",
    "                    print(error)\n",
    "                    continue\n",
    "                tracker.insert(\n",
    "                    NERTrackerRow(\n",
    "                        start, model, dataset, config.tokenizer_model, \"entity-from-train+dev\", config.output_folder, query_time(trainer.train),\n",
    "                        results[\"f1-score\"], results[\"precision-score\"], results[\"recall-score\"], results[\"epoch_count\"], results[\"test-loss\"], tag\n",
    "                    )\n",
    "                )\n",
    "                tracker.save(labpath)\n",
    "                del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Given models, datasets, gazetteers, train them\n",
    "You can find all available models, datasets and gazetteers by the following code block.\n",
    "\n",
    "```python\n",
    "models = gctx.get_model_names()\n",
    "datasets = gctx.get_dataset_names()\n",
    "gazetteers = gctx.get_gazetteer_names()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labpath = \"../data/logs/experiment_gazetteer_modification[train+dev]_with_types.csv\"\n",
    "cfgdir = \"../configs\"\n",
    "models = [\"mdgg\", \"ses\", \"cgn\"]\n",
    "datasets = [\"chip\"]\n",
    "n = 5\n",
    "tag = \"gazetteer_modification\"\n",
    "kwargs = {\"data_folder\": \"../data\"}\n",
    "\n",
    "trainall(labpath, cfgdir, models, datasets, n, tag, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
