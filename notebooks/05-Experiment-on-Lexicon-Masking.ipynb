{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Experiment: Lexicon Masking\n",
    "This notebook evaluate the test set for the task `Lexicon Masking`.\n",
    "\n",
    "**Note**: Before conducting experiments, you need to install `kaner` package first. Otherwise, this notebook will raise an *import error*.\n",
    "\n",
    "```bash\n",
    "cd ../\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "import pprint\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from kaner.context import GlobalContext as gctx\n",
    "from kaner.adapter.tokenizer import CharTokenizer\n",
    "from kaner.adapter.knowledge import Gazetteer\n",
    "from kaner.adapter.in_adapter import split_dataset\n",
    "from kaner.trainer import NERTrainer, TrainerConfig\n",
    "from kaner.common import load_json, load_jsonl, save_json\n",
    "\n",
    "\n",
    "gctx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matlexs(datasets: List[dict], gazetteer: Gazetteer, mode: str) -> List[set]:\n",
    "    \"\"\"\n",
    "    Given a matching mode, return all matched lexicons.\n",
    "    \"\"\"\n",
    "    max_seq_len = 512\n",
    "    assert mode in [\"all\", \"entity\", \"non-entity\"]\n",
    "    # get all spans\n",
    "    all_spans = set()\n",
    "    for dataset in datasets:\n",
    "        for datapoint in dataset:\n",
    "            for span in datapoint[\"spans\"]:\n",
    "                all_spans.add(span[\"text\"])\n",
    "    # get all matched lexicons\n",
    "    matched_lexicons = []\n",
    "    for dataset in datasets:\n",
    "        lexicons = set()\n",
    "        for datapoint in dataset:\n",
    "            tokens = list(datapoint[\"text\"])[:max_seq_len]\n",
    "            for i, _ in enumerate(tokens):\n",
    "                items = gazetteer.search(tokens[i:])\n",
    "                if mode == \"all\":\n",
    "                    lexicons.update(items)\n",
    "                else:\n",
    "                    for item in items:\n",
    "                        if mode == \"entity\":\n",
    "                            if item in all_spans:\n",
    "                                lexicons.add(item)\n",
    "                        elif mode == \"non-entity\":\n",
    "                            if item not in all_spans:\n",
    "                                lexicons.add(item)\n",
    "        matched_lexicons.append(lexicons)\n",
    "    \n",
    "    return matched_lexicons\n",
    "\n",
    "\n",
    "def evaluate(model_folder: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate all settings.\n",
    "    \"\"\"\n",
    "    options = load_json(\"utf-8\", model_folder, \"config.json\")\n",
    "    options[\"output_folder\"] = model_folder\n",
    "    options[\"identity\"] = os.path.basename(os.path.normpath(model_folder))\n",
    "    config = TrainerConfig(options, data_folder=\"../data\")\n",
    "    tokenizer = CharTokenizer(model_folder)\n",
    "    gazetteer = Gazetteer(model_folder)\n",
    "    datasets = split_dataset(config.dataset_folder)\n",
    "    out_adapter = gctx.create_outadapter(config.out_adapter, dataset_folder=model_folder, file_name=\"labels\")\n",
    "    collate_fn = gctx.create_batcher(\n",
    "        config.model, input_pad=tokenizer.pad_id, output_pad=out_adapter.unk_id, lexicon_pad=gazetteer.pad_id, device=config.device\n",
    "    )\n",
    "    model = gctx.create_model(config.model, **config.hyperparameters)\n",
    "\n",
    "    result = {}\n",
    "    # IS Intervention\n",
    "    A, _, B = get_matlexs(datasets, gazetteer, \"all\")\n",
    "    I = A.intersection(B)\n",
    "    S = A.union(B) - A\n",
    "    gazetteer.mask(list(I), True)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in [[], [], datasets[2]]\n",
    "    )\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "    result[\"I\"] = trainer._test(trainer._test_loader)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gazetteer.mask(list(I), False)\n",
    "\n",
    "    gazetteer.mask(list(S), True)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in [[], [], datasets[2]]\n",
    "    )\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "    result[\"S\"] = trainer._test(trainer._test_loader)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gazetteer.mask(list(S), False)\n",
    "\n",
    "    # Entity vs. Non-Entity\n",
    "    _, _, E = get_matlexs(datasets, gazetteer, \"entity\")\n",
    "    _, _, N = get_matlexs(datasets, gazetteer, \"non-entity\")\n",
    "    gazetteer.mask(list(E), True)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in [[], [], datasets[2]]\n",
    "    )\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "    result[\"E\"] = trainer._test(trainer._test_loader)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gazetteer.mask(list(E), False)\n",
    "\n",
    "    gazetteer.mask(list(N), True)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in [[], [], datasets[2]]\n",
    "    )\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "    result[\"N\"] = trainer._test(trainer._test_loader)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gazetteer.mask(list(N), False)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute `do` Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 0...........................\n",
      "## Log 1...........................\n",
      "## Log 2...........................\n",
      "## Log 3...........................\n",
      "## Log 4...........................\n",
      "## Log 5...........................\n",
      "## Log 6...........................\n",
      "## Log 7...........................\n",
      "## Log 8...........................\n",
      "## Log 9...........................\n",
      "## Log 10...........................\n",
      "## Log 11...........................\n",
      "## Log 12...........................\n",
      "## Log 13...........................\n",
      "## Log 14...........................\n",
      "## Log 15...........................\n",
      "## Log 16...........................\n",
      "## Log 17...........................\n",
      "## Log 18...........................\n",
      "## Log 19...........................\n",
      "## Log 20...........................\n",
      "## Log 21...........................\n",
      "## Log 22...........................\n",
      "## Log 23...........................\n",
      "## Log 24...........................\n",
      "## Log 25...........................\n",
      "## Log 26...........................\n",
      "## Log 27...........................\n",
      "## Log 28...........................\n",
      "## Log 29...........................\n",
      "## Log 30...........................\n",
      "## Log 31...........................\n",
      "## Log 32...........................\n",
      "## Log 33...........................\n",
      "## Log 34...........................\n",
      "## Log 35...........................\n",
      "## Log 36...........................\n",
      "## Log 37...........................\n",
      "## Log 38...........................\n",
      "## Log 39...........................\n",
      "## Log 40...........................\n",
      "## Log 41...........................\n",
      "## Log 42...........................\n",
      "## Log 43...........................\n",
      "## Log 44...........................\n",
      "## Log 45...........................\n",
      "## Log 46...........................\n",
      "## Log 47...........................\n",
      "## Log 48...........................\n",
      "## Log 49...........................\n",
      "## Log 50...........................\n",
      "## Log 51...........................\n",
      "## Log 52...........................\n",
      "## Log 53...........................\n",
      "## Log 54...........................\n",
      "## Log 55...........................\n",
      "## Log 56...........................\n",
      "## Log 57...........................\n",
      "## Log 58...........................\n",
      "## Log 59...........................\n",
      "## Log 60...........................\n",
      "## Log 61...........................\n",
      "## Log 62...........................\n",
      "## Log 63...........................\n",
      "## Log 64...........................\n",
      "## Log 65...........................\n",
      "## Log 66...........................\n",
      "## Log 67...........................\n",
      "## Log 68...........................\n",
      "## Log 69...........................\n",
      "## Log 70...........................\n",
      "## Log 71...........................\n",
      "## Log 72...........................\n",
      "## Log 73...........................\n",
      "## Log 74...........................\n",
      "## Log 75...........................\n",
      "## Log 76...........................\n",
      "## Log 77...........................\n",
      "## Log 78...........................\n",
      "## Log 79...........................\n",
      "## Log 80...........................\n",
      "## Log 81...........................\n",
      "## Log 82...........................\n",
      "## Log 83...........................\n",
      "## Log 84...........................\n",
      "## Log 85...........................\n",
      "## Log 86...........................\n",
      "## Log 87...........................\n",
      "## Log 88...........................\n",
      "## Log 89...........................\n",
      "## Log 90...........................\n",
      "## Log 91...........................\n",
      "## Log 92...........................\n",
      "## Log 93...........................\n",
      "## Log 94...........................\n",
      "## Log 95...........................\n",
      "## Log 96...........................\n",
      "## Log 97...........................\n",
      "## Log 98...........................\n",
      "## Log 99...........................\n",
      "## Log 100...........................\n",
      "## Log 101...........................\n",
      "## Log 102...........................\n",
      "## Log 103...........................\n",
      "## Log 104...........................\n",
      "## Log 105...........................\n",
      "## Log 106...........................\n",
      "## Log 107...........................\n",
      "## Log 108...........................\n",
      "## Log 109...........................\n",
      "## Log 110...........................\n",
      "## Log 111...........................\n",
      "## Log 112...........................\n",
      "## Log 113...........................\n",
      "## Log 114...........................\n",
      "## Log 115...........................\n",
      "## Log 116...........................\n",
      "## Log 117...........................\n",
      "## Log 118...........................\n",
      "## Log 119...........................\n",
      "## Log 120...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-1\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5335.05it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2299.25it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2496.64it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4638.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 121...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-2\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2990.25it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2307.20it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2450.91it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4641.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 122...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-3\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2939.01it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2320.42it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2456.99it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4622.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 123...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-4\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2946.26it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2298.35it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2491.44it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4621.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 124...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-5\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2915.35it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2300.51it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2446.66it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4594.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 125...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-6\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5378.70it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:02<00:00, 2163.22it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2373.94it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4471.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 126...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-7\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5389.93it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2212.31it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2382.59it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4445.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 127...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-8\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5411.82it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2197.80it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2410.78it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4484.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 128...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-9\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5394.48it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2202.92it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2383.24it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4386.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 129...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-10\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5416.60it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:02<00:00, 2153.01it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:01<00:00, 2365.51it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4491.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 130...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-11\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 7009.45it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5188.54it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5393.94it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 6272.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 131...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-12\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5306.03it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4990.31it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5251.01it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 6348.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 132...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-13\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5292.46it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5053.26it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5354.22it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 6210.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 133...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-14\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5305.15it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 4997.94it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5250.31it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 6334.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 134...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-cgn-msraner-15\n",
      "[Dataset: ../data/datahub/msraner] 41728 train, 4636 dev, 4365 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5305.06it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5102.34it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 5301.12it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4365/4365 [00:00<00:00, 6336.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 135...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-1\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1622.93it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1329.92it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1348.78it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1534.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 136...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-2\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1339.74it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1314.06it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1352.01it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1540.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 137...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-3\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1316.03it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1326.99it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1347.11it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1541.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 138...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-4\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1315.67it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1330.24it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1357.36it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1562.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 139...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-5\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1317.44it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1315.66it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1345.64it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1541.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 140...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-6\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:02<00:00, 1648.61it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1286.82it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1320.42it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1499.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 141...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-7\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:02<00:00, 1647.96it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1298.64it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1322.55it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1504.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 142...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-8\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1641.54it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1297.34it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1316.41it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1509.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 143...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-9\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1632.86it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1291.68it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1317.48it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1515.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 144...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-ses-ontonotes-11\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1627.02it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1301.79it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1317.44it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1510.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 145...........................\n",
      "Lexicon embedding is None! ../data/logs/trainer-ses-ontonotes-16\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:01<00:00, 2508.36it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1434.15it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1451.53it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1471.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 146...........................\n",
      "Lexicon embedding is None! ../data/logs/trainer-ses-ontonotes-18\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1401.58it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1430.56it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1449.49it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1469.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 147...........................\n",
      "Lexicon embedding is None! ../data/logs/trainer-ses-ontonotes-19\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1397.28it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1432.01it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1431.91it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1450.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 148...........................\n",
      "Lexicon embedding is None! ../data/logs/trainer-ses-ontonotes-20\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1393.36it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1400.09it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1425.30it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1449.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 149...........................\n",
      "Lexicon embedding is None! ../data/logs/trainer-ses-ontonotes-21\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1402.42it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1415.43it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1427.63it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [00:03<00:00, 1457.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 150...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-mdgg-ontonotes-1\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:27<00:00,  8.69it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:25<00:00,  8.71it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:25<00:00,  8.72it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:27<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 151...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-mdgg-ontonotes-2\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:52<00:00,  8.32it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:53<00:00,  8.30it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:50<00:00,  8.35it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:50<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 152...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-mdgg-ontonotes-3\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:58<00:00,  8.23it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [10:00<00:00,  8.21it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:56<00:00,  8.26it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:59<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 153...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-mdgg-ontonotes-4\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:57<00:00,  8.25it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [10:01<00:00,  8.20it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [10:02<00:00,  8.18it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:57<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 154...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-mdgg-ontonotes-5\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:56<00:00,  8.26it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:55<00:00,  8.28it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:57<00:00,  8.26it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [09:54<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Log 155...........................\n",
      "Lexicon embedding is None! .././data/logs/trainer-mdgg-ontonotes-6\n",
      "[Dataset: ../data/datahub/ontonotes] 39446 train, 4930 dev, 4930 test. (resplit: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [19:48<00:00,  4.15it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [19:58<00:00,  4.11it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4930/4930 [19:33<00:00,  4.20it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor: 0it [00:00, ?it/s]\n",
      "Text2Tensor:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3920/4930 [15:07<03:52,  4.35it/s]"
     ]
    }
   ],
   "source": [
    "def load_experiments(folder: str = \"../data/logs\") -> List[dict]:\n",
    "    file_path = os.path.join(folder, \"experiments.csv\")\n",
    "    logs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        line = fin.readline()\n",
    "        columns = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            log = {k: v for k, v in zip(columns, line.replace(\"\\n\", \"\").split(\",\"))}\n",
    "            if log[\"model\"] not in [\"blcrf\", \"plmtg\"]:\n",
    "                logs.append(log)\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "dolog_path = os.path.join(\"../data\", \"do_full_logs.json\")\n",
    "if os.path.isfile(dolog_path):\n",
    "    logs = load_json(\"utf-8\", dolog_path)\n",
    "else:\n",
    "    logs = load_experiments()\n",
    "for i, _ in enumerate(logs):\n",
    "    print(\"## Log {0}...........................\".format(i))\n",
    "    if \"do\" in logs[i].keys():\n",
    "        continue\n",
    "    if not logs[i][\"log_dir\"].startswith(\"../\"):\n",
    "        folder = os.path.join(\"../\", logs[i][\"log_dir\"])\n",
    "    else:\n",
    "        folder = logs[i][\"log_dir\"]\n",
    "    folder = folder.replace(\"tmp/\", \"\")\n",
    "    logs[i][\"do\"] = evaluate(folder)\n",
    "    save_json(logs, dolog_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
