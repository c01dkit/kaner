{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Experiment: Gazetteer Attributes\n",
    "This notebook evaluate the test set for the task `Gazetteer Attributes`.\n",
    "\n",
    "**Note**: Before conducting experiments, you need to install `kaner` package first. Otherwise, this notebook will raise an *import error*.\n",
    "\n",
    "```bash\n",
    "cd ../\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from kaner.context import GlobalContext as gctx\n",
    "from kaner.adapter.tokenizer import CharTokenizer\n",
    "from kaner.adapter.knowledge import Gazetteer\n",
    "from kaner.adapter.in_adapter import split_dataset\n",
    "from kaner.adapter.out_adapter import BaseOutAdapter\n",
    "from kaner.trainer import NERTrainer, TrainerConfig\n",
    "from kaner.tracker import NERTracker, NERTrackerRow\n",
    "from kaner.common import load_json, load_jsonl, save_json\n",
    "from kaner.common.func import query_time\n",
    "\n",
    "\n",
    "gctx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gazetteer Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_gazetteer_by_pp(lexicon_pp: float, gazetteer_folder: str) -> Gazetteer:\n",
    "    assert isinstance(lexicon_pp, float) and 0. < lexicon_pp <= 1.0\n",
    "    gazetteer = Gazetteer(gazetteer_folder)\n",
    "    lexicons = []\n",
    "    with open(os.path.join(gazetteer_folder, \"lexicons.txt\"), \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            lexicon = line.replace(\"\\n\", \"\").split(\"\\t\")[0]\n",
    "            if lexicon == \"[PAD]\":\n",
    "                continue\n",
    "            lexicons.append(lexicon)\n",
    "    base_length = int(len(lexicons)*lexicon_pp)\n",
    "    masked_lexicons = list(set(lexicons) - set(lexicons[:base_length]))\n",
    "    print(\"Total: {0}, Remaining: {1}, Masked: {2}\".format(len(lexicons), base_length, len(masked_lexicons)))\n",
    "    gazetteer.mask(masked_lexicons, True)\n",
    "\n",
    "    return gazetteer\n",
    "\n",
    "\n",
    "def train(config: TrainerConfig, lexicon_pp: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a configuration, train a model on a dataset with gazetteer modification.\n",
    "\n",
    "    Args:\n",
    "        config (TrainerConfig): Trainer Configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_hyperparameters(tokenizer: CharTokenizer, out_adapter: BaseOutAdapter, gazetteer: Gazetteer):\n",
    "        \"\"\"\n",
    "        Update hyper parameters.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (CharTokenizer): Tokenizer.\n",
    "            out_adapter (BaseOutAdapter): Output adapter.\n",
    "            gazetteer (Gazetteer): Gazetteer.\n",
    "        \"\"\"\n",
    "        partial_configs = {\"n_tags\": len(out_adapter)}\n",
    "        partial_configs.update(tokenizer.configs())\n",
    "        partial_configs.update(gazetteer.configs())\n",
    "\n",
    "        return partial_configs\n",
    "\n",
    "    raw_datasets = split_dataset(config.dataset_folder, dataset_pp=config.dataset_pp)\n",
    "    tokenizer = CharTokenizer(config.tokenizer_model_folder)\n",
    "    tokenizer.save(config.output_folder)\n",
    "    gazetteer = get_masked_gazetteer_by_pp(lexicon_pp, config.gazetteer_model_folder)\n",
    "    gazetteer.save(config.output_folder)\n",
    "    out_adapter = gctx.create_outadapter(config.out_adapter, dataset_folder=config.dataset_folder, file_name=\"labels\")\n",
    "    out_adapter.save(config.output_folder, \"labels\")\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in raw_datasets\n",
    "    )\n",
    "    token_embeddings = tokenizer.embeddings()\n",
    "    lexicon_embeddings = gazetteer.embeddings()\n",
    "    config.hyperparameters = update_hyperparameters(tokenizer, out_adapter, gazetteer)\n",
    "    collate_fn = gctx.create_batcher(\n",
    "        config.model, input_pad=tokenizer.pad_id, output_pad=out_adapter.unk_id, lexicon_pad=gazetteer.pad_id, device=config.device\n",
    "    )\n",
    "    model = gctx.create_model(config.model, **config.hyperparameters, token_embeddings=token_embeddings, lexicon_embeddings=lexicon_embeddings)\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, nn.CrossEntropyLoss(),\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    results = trainer.train()\n",
    "\n",
    "    return results, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainall(labpath: str, cfgdir: str, m: List[str], d: List[str], n: int, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Experiments for all model's training.\n",
    "\n",
    "    Args:\n",
    "        labpath (str): The file path of recording experimental results.\n",
    "        cfgdir (str): Configuration folder.\n",
    "        m (List[str]): All specific models to be trained.\n",
    "        d (List[str]): All specific datasets to be tested.\n",
    "        n (int): The number of training repeating times.\n",
    "        tag (str): Experimental tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_names(names: List[str], all_names: List[str], name_type: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Check whether the name that user inputs is correct.\n",
    "\n",
    "        Args:\n",
    "            names (List[str]): The names (dataset, model, gazetteer) that user inputs.\n",
    "            all_names (List[str]): All names (dataset, model, gazetteer) that this libary provides.\n",
    "            name_type (str): The type of the name (Dataset, Model, Gazetteer).\n",
    "        \"\"\"\n",
    "        if len(names) == 0:\n",
    "            names = all_names\n",
    "        else:\n",
    "            for name in names:\n",
    "                if name not in all_names:\n",
    "                    print(\"[{0}] {1} is not in {2}\".format(name_type, name, all_names))\n",
    "                    exit(0)\n",
    "        return names\n",
    "\n",
    "    tracker = NERTracker.load(labpath)\n",
    "    models = update_names(m, gctx.get_model_names(), \"Model\")\n",
    "    datasets = update_names(d, gctx.get_dataset_names(), \"Dataset\")\n",
    "\n",
    "    print(\"--------------------- Laboratory Configuration ---------------------\")\n",
    "    print(\"Models: {0}\".format(models))\n",
    "    print(\"Datasets: {0}\".format(datasets))\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            for _ in range(n):\n",
    "                for pp in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "                    tag = \"lexicon-pp:{0}\".format(pp)\n",
    "                    if len(tracker.query(dataset=dataset, model=model, tag=tag)) >= n:\n",
    "                        continue\n",
    "                    config = TrainerConfig(os.path.join(cfgdir, model + \".yml\"), dataset=dataset, **kwargs)\n",
    "                    start = str(datetime.now())\n",
    "                    try:\n",
    "                        results, trainer = train(config, pp)\n",
    "                    except RuntimeError as error:\n",
    "                        print(error)\n",
    "                        continue\n",
    "                    tracker.insert(\n",
    "                        NERTrackerRow(\n",
    "                            start, model, dataset, config.tokenizer_model, config.gazetteer_model, config.output_folder, query_time(trainer.train),\n",
    "                            results[\"f1-score\"], results[\"precision-score\"], results[\"recall-score\"], results[\"epoch_count\"], results[\"test-loss\"], tag\n",
    "                        )\n",
    "                    )\n",
    "                    tracker.save(labpath)\n",
    "                    del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Laboratory Configuration ---------------------\n",
      "Models: ['ses', 'cgn', 'mdgg']\n",
      "Datasets: ['weiboner']\n",
      "--------------------------------------------------------------------\n",
      "[Dataset: ../data/datahub/weiboner] 1350 train, 270 dev, 270 test. (resplit: False)\n",
      "Total: 704368, Remaining: 140873, Masked: 563495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1350/1350 [00:00<00:00, 1834.29it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [00:00<00:00, 1828.44it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [00:03<00:00, 83.16it/s]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.91it/s]\n",
      "2020-11-19 06:27:44 [ses, weiboner] epoch: 0, no_improvement: 0, dev-f1: 0.16335, dev-precision: 0.36283, dev-recall: 0.1054, dev-loss: 11.10511, train-loss: 19.79544\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.83it/s]\n",
      "2020-11-19 06:27:51 [ses, weiboner] epoch: 1, no_improvement: 0, dev-f1: 0.42493, dev-precision: 0.47319, dev-recall: 0.3856, dev-loss: 7.98017, train-loss: 9.97929\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.94it/s]\n",
      "2020-11-19 06:27:59 [ses, weiboner] epoch: 2, no_improvement: 0, dev-f1: 0.51223, dev-precision: 0.51289, dev-recall: 0.51157, dev-loss: 7.14867, train-loss: 6.49991\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.95it/s]\n",
      "2020-11-19 06:28:06 [ses, weiboner] epoch: 3, no_improvement: 0, dev-f1: 0.52459, dev-precision: 0.51485, dev-recall: 0.5347, dev-loss: 6.74358, train-loss: 4.15166\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.93it/s]\n",
      "2020-11-19 06:28:12 [ses, weiboner] epoch: 4, no_improvement: 1, dev-f1: 0.51458, dev-precision: 0.5075, dev-recall: 0.52185, dev-loss: 7.17663, train-loss: 2.83107\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.91it/s]\n",
      "2020-11-19 06:28:19 [ses, weiboner] epoch: 5, no_improvement: 2, dev-f1: 0.49624, dev-precision: 0.48411, dev-recall: 0.509, dev-loss: 7.98212, train-loss: 2.66809\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.93it/s]\n",
      "2020-11-19 06:28:26 [ses, weiboner] epoch: 6, no_improvement: 0, dev-f1: 0.52836, dev-precision: 0.71491, dev-recall: 0.41902, dev-loss: 9.59721, train-loss: 2.50147\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.94it/s]\n",
      "2020-11-19 06:28:34 [ses, weiboner] epoch: 7, no_improvement: 0, dev-f1: 0.55194, dev-precision: 0.69531, dev-recall: 0.45758, dev-loss: 7.80414, train-loss: 2.33709\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.92it/s]\n",
      "2020-11-19 06:28:41 [ses, weiboner] epoch: 8, no_improvement: 0, dev-f1: 0.56652, dev-precision: 0.63871, dev-recall: 0.509, dev-loss: 6.56037, train-loss: 1.45055\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.94it/s]\n",
      "2020-11-19 06:28:47 [ses, weiboner] epoch: 9, no_improvement: 1, dev-f1: 0.55085, dev-precision: 0.61129, dev-recall: 0.50129, dev-loss: 7.03194, train-loss: 0.87876\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:06<00:00,  3.56it/s]\n",
      "2020-11-19 06:28:55 [ses, weiboner] epoch: 10, no_improvement: 2, dev-f1: 0.55072, dev-precision: 0.51936, dev-recall: 0.58612, dev-loss: 7.86007, train-loss: 0.59061\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.67it/s]\n",
      "2020-11-19 06:29:01 [ses, weiboner] epoch: 11, no_improvement: 3, dev-f1: 0.54937, dev-precision: 0.54115, dev-recall: 0.55784, dev-loss: 8.08487, train-loss: 0.56261\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.73it/s]\n",
      "2020-11-19 06:29:08 [ses, weiboner] epoch: 12, no_improvement: 4, dev-f1: 0.55043, dev-precision: 0.62623, dev-recall: 0.491, dev-loss: 9.05202, train-loss: 0.51773\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.74it/s]\n",
      "2020-11-19 06:29:15 [ses, weiboner] epoch: 13, no_improvement: 5, dev-f1: 0.54711, dev-precision: 0.66914, dev-recall: 0.46272, dev-loss: 9.54563, train-loss: 0.34786\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.97it/s]\n",
      "2020-11-19 06:29:22 [ses, weiboner] epoch: 14, no_improvement: 0, dev-f1: 0.57103, dev-precision: 0.6231, dev-recall: 0.52699, dev-loss: 8.80856, train-loss: 0.26998\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.99it/s]\n",
      "2020-11-19 06:29:29 [ses, weiboner] epoch: 15, no_improvement: 1, dev-f1: 0.55491, dev-precision: 0.63366, dev-recall: 0.49357, dev-loss: 9.28209, train-loss: 0.19625\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.97it/s]\n",
      "2020-11-19 06:29:35 [ses, weiboner] epoch: 16, no_improvement: 2, dev-f1: 0.5616, dev-precision: 0.6343, dev-recall: 0.50386, dev-loss: 9.45318, train-loss: 0.16812\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.69it/s]\n",
      "2020-11-19 06:29:42 [ses, weiboner] epoch: 17, no_improvement: 3, dev-f1: 0.55252, dev-precision: 0.62745, dev-recall: 0.49357, dev-loss: 9.6349, train-loss: 0.15502\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.91it/s]\n",
      "2020-11-19 06:29:49 [ses, weiboner] epoch: 18, no_improvement: 4, dev-f1: 0.55747, dev-precision: 0.63192, dev-recall: 0.49871, dev-loss: 9.79565, train-loss: 0.14663\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.95it/s]\n",
      "2020-11-19 06:29:56 [ses, weiboner] epoch: 19, no_improvement: 5, dev-f1: 0.55747, dev-precision: 0.63192, dev-recall: 0.49871, dev-loss: 9.90378, train-loss: 0.13871\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.99it/s]\n",
      "2020-11-19 06:30:02 [ses, weiboner] epoch: 20, no_improvement: 6, dev-f1: 0.55954, dev-precision: 0.63312, dev-recall: 0.50129, dev-loss: 9.99898, train-loss: 0.13377\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.91it/s]\n",
      "2020-11-19 06:30:09 [ses, weiboner] epoch: 21, no_improvement: 7, dev-f1: 0.55874, dev-precision: 0.63107, dev-recall: 0.50129, dev-loss: 10.07959, train-loss: 0.12868\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  4.02it/s]\n",
      "2020-11-19 06:30:15 [ses, weiboner] epoch: 22, no_improvement: 8, dev-f1: 0.55874, dev-precision: 0.63107, dev-recall: 0.50129, dev-loss: 10.14502, train-loss: 0.1246\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  4.06it/s]\n",
      "2020-11-19 06:30:21 [ses, weiboner] epoch: 23, no_improvement: 9, dev-f1: 0.55587, dev-precision: 0.62783, dev-recall: 0.49871, dev-loss: 10.19753, train-loss: 0.12088\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  4.04it/s]\n",
      "2020-11-19 06:30:28 [ses, weiboner] epoch: 24, no_improvement: 10, dev-f1: 0.55508, dev-precision: 0.62581, dev-recall: 0.49871, dev-loss: 10.23994, train-loss: 0.11767\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.99it/s]\n",
      "2020-11-19 06:30:34 [ses, weiboner] epoch: 25, no_improvement: 11, dev-f1: 0.55508, dev-precision: 0.62581, dev-recall: 0.49871, dev-loss: 10.27227, train-loss: 0.11475\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.99it/s]\n",
      "2020-11-19 06:30:41 [ses, weiboner] epoch: 26, no_improvement: 12, dev-f1: 0.55508, dev-precision: 0.62581, dev-recall: 0.49871, dev-loss: 10.30109, train-loss: 0.11238\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.94it/s]\n",
      "2020-11-19 06:30:47 [ses, weiboner] epoch: 27, no_improvement: 13, dev-f1: 0.55508, dev-precision: 0.62581, dev-recall: 0.49871, dev-loss: 10.31964, train-loss: 0.11026\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.91it/s]\n",
      "2020-11-19 06:30:54 [ses, weiboner] epoch: 28, no_improvement: 14, dev-f1: 0.55429, dev-precision: 0.62379, dev-recall: 0.49871, dev-loss: 10.33314, train-loss: 0.10858\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.73it/s]\n",
      "2020-11-19 06:31:01 [ses, weiboner] epoch: 29, no_improvement: 15, dev-f1: 0.55429, dev-precision: 0.62379, dev-recall: 0.49871, dev-loss: 10.34124, train-loss: 0.10724\n",
      "2020-11-19 06:31:02 [ses, weiboner] {\"f1-score\": 0.5764546684709065, \"precision-score\": 0.6553846153846153, \"recall-score\": 0.5144927536231884, \"test-loss\": 10.639748668670654, \"epoch_count\": 30}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timing] kaner.trainer.base.train function took 208.714 sec\n",
      "# Save experimental data into ../data/logs/experiments_gazetteer_size.csv\n",
      "[Dataset: ../data/datahub/weiboner] 1350 train, 270 dev, 270 test. (resplit: False)\n",
      "Total: 704368, Remaining: 281747, Masked: 422621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1350/1350 [00:03<00:00, 357.78it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [00:00<00:00, 1730.86it/s]\n",
      "Text2Tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [00:00<00:00, 1767.22it/s]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:09<00:00,  2.41it/s]\n",
      "2020-11-19 06:31:28 [ses, weiboner] epoch: 0, no_improvement: 0, dev-f1: 0.18785, dev-precision: 0.33117, dev-recall: 0.13111, dev-loss: 11.84804, train-loss: 20.37469\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.96it/s]\n",
      "2020-11-19 06:31:35 [ses, weiboner] epoch: 1, no_improvement: 0, dev-f1: 0.41252, dev-precision: 0.46178, dev-recall: 0.37275, dev-loss: 8.19686, train-loss: 10.16248\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.90it/s]\n",
      "2020-11-19 06:31:42 [ses, weiboner] epoch: 2, no_improvement: 0, dev-f1: 0.48969, dev-precision: 0.49096, dev-recall: 0.48843, dev-loss: 7.26791, train-loss: 6.87067\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.95it/s]\n",
      "2020-11-19 06:31:49 [ses, weiboner] epoch: 3, no_improvement: 0, dev-f1: 0.52081, dev-precision: 0.54494, dev-recall: 0.49871, dev-loss: 6.87594, train-loss: 4.45781\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:05<00:00,  3.99it/s]\n"
     ]
    }
   ],
   "source": [
    "labpath = \"../data/logs/experiments_gazetteer_size.csv\"\n",
    "cfgdir = \"../configs\"\n",
    "models = [\"ses\", \"cgn\", \"mdgg\"]\n",
    "datasets = [\"weiboner\"]\n",
    "n = 1\n",
    "kwargs = {\"data_folder\": \"../data\"}\n",
    "\n",
    "trainall(labpath, cfgdir, models, datasets, n, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
